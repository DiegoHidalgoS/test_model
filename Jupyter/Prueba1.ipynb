{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7de51c4",
   "metadata": {},
   "source": [
    "# Se importan los paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e7faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shapefile as shp\n",
    "# import pyshp as shp\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import dates as mpl_dates\n",
    "import flopy\n",
    "from flopy.utils.gridintersect import GridIntersect\n",
    "from flopy.utils.gridgen import Gridgen\n",
    "from flopy.plot.map import PlotMapView\n",
    "from csv import reader\n",
    "from csv import DictReader\n",
    "import datetime\n",
    "import shapely\n",
    "from shapely.geometry import Polygon, Point, LineString, MultiLineString, MultiPoint, MultiPolygon\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "\n",
    "from osgeo import gdal\n",
    "import geopandas\n",
    "\n",
    "\n",
    "from flopy.utils.postprocessing import get_transmissivities, get_water_table, get_gradients, get_saturated_thickness\n",
    "import flopy.utils.binaryfile as bf\n",
    "\n",
    "from flopy.utils import Raster\n",
    "from flopy.utils import ZoneBudget\n",
    "\n",
    "from scipy import interpolate\n",
    "from scipy.interpolate import interpn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a84d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0x = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204a1084",
   "metadata": {},
   "source": [
    "# se crea directorio en caso de no existir para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93874843",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../Modelos'):\n",
    "    os.makedirs('../Modelos')\n",
    "\n",
    "os.chdir('../Modelos')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45facda",
   "metadata": {},
   "source": [
    "se crea el directorio del modelo, y de los resultados y la ruta relacionada al ejecutable (modflow y gidgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908f6c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'mnsda'\n",
    "model_ws = os.path.join('.', modelname)\n",
    "if not os.path.exists(model_ws):\n",
    "    os.makedirs(model_ws)\n",
    "    \n",
    "earr_ws = os.path.join(model_ws, 'ext_arrays')\n",
    "if not os.path.exists(earr_ws):\n",
    "    os.makedirs(earr_ws)\n",
    "    \n",
    "postproc_ws = os.path.join(model_ws, 'post_proc')\n",
    "if not os.path.exists(postproc_ws):\n",
    "    os.makedirs(postproc_ws)\n",
    "    \n",
    "#gridgen_exe = flopy.which('gridgen')\n",
    "#mfusg_exe = flopy.which('USGs_1')\n",
    "\n",
    "mfusg_exe=os.path.join('..','exe','USGs_1.exe')\n",
    "gridgen_exe=os.path.join('..','exe','gridgen')\n",
    "\n",
    "if mfusg_exe is None:\n",
    "    msg = ('Warning, mfusg is not in your path. '\n",
    "    'provide a full path to the mf6 binary executable.')\n",
    "    print(msg)\n",
    "else:\n",
    "    print('Mfusg fue encontrado en: {}'.format(mfusg_exe))\n",
    "    \n",
    "if gridgen_exe is None:\n",
    "    msg = ('Warning, gridgen is not in your path. '\n",
    "    'provide a full path to the binary executable.')\n",
    "    print(msg)\n",
    "else:\n",
    "    print('Gridgen fue encontrado en: {}'.format(gridgen_exe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9eba3c",
   "metadata": {},
   "source": [
    "# Se define largo ancho y capas del modelo\n",
    "\n",
    "rango eje x: 545700 - 593060\n",
    "rango eje y: 7377200 - 7422000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e8fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a large domain is defined\n",
    "Lx = -(545700 - 593060)\n",
    "Ly = -(7377200 - 7422000)\n",
    "\n",
    "gl_files = ['Acuitardo_techo_2.asc','UC_techo.asc','UD_techo.asc','Basamento_V2_techo.asc']\n",
    "\n",
    "# [ UA , UB , UC , UD , BASAM]\n",
    "# Number of model layers\n",
    "gls_lays = np.array([2,2,4,1,1]) # numero de capas entre capas guias\n",
    "nlay = gls_lays.sum()\n",
    "\n",
    "#Que capa corresponde al top de cada unidad relevante\n",
    "layerUA_top=1\n",
    "\n",
    "layerUB_top=gls_lays[0]+layerUA_top\n",
    "\n",
    "layerUC_top=gls_lays[1]+layerUB_top\n",
    "\n",
    "\n",
    "# Base model of 100x100 row and col\n",
    "nrow = 35\n",
    "ncol = 37\n",
    "# Horizontal discretization\n",
    "delr = Lx / ncol\n",
    "delc = Ly / nrow\n",
    "\n",
    "#periodos de estres\n",
    "nper = 72\n",
    "\n",
    "# origins\n",
    "xul = 545700\n",
    "yul = 7377200+delr*nrow\n",
    "\n",
    "\n",
    "#cellxy x,y como tupla\n",
    "cellxy = np.empty((nrow*ncol, 2), dtype=float)\n",
    "inode = 0\n",
    "for irow in range(nrow):\n",
    "    for icol in range(ncol):\n",
    "        cellxy[inode,0]=xul+delc*(icol+1/2)\n",
    "        cellxy[inode,1]=yul-delr*(irow+1/2)\n",
    "        inode=inode+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b0d5ef",
   "metadata": {},
   "source": [
    "# Geometría \n",
    "Se construye la geometría según las capas guías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98603afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "topm = np.zeros((1, nrow, ncol), dtype=np.float32)\n",
    "\n",
    "topo_file = 'DEM.asc'\n",
    "\n",
    "pth = os.path.join(\"./geometrias\",topo_file)\n",
    "with open(pth,'r') as f:\n",
    "    lines = f.readlines()\n",
    "f.close()\n",
    "    \n",
    "asc_grid = np.loadtxt(os.path.join(\"./geometrias\",topo_file), skiprows=6)\n",
    "\n",
    "spatial_data = [] # ncols, nrows, xllcorner, yllcorner, dx, dy\n",
    "    \n",
    "for i in range(6):\n",
    "    spatial_data.append(float(lines[i].split()[1]))\n",
    "\n",
    "ncols = int(spatial_data[0])\n",
    "nrows = int(spatial_data[1])\n",
    "xllcorner = spatial_data[2]\n",
    "yllcorner = spatial_data[3]\n",
    "idelr = spatial_data[5]\n",
    "idelc = spatial_data[4]\n",
    "    \n",
    "asc_grid = np.reshape(asc_grid,nrows*ncols)\n",
    "    \n",
    "truth_xy = np.empty((nrows*ncols, 2), dtype=float)\n",
    "inode=0\n",
    "for irow in range(nrows):\n",
    "    for icol in range(ncols):\n",
    "        truth_xy[inode,0]=xllcorner+idelc*(icol+1/2)\n",
    "        truth_xy[inode,1]=yllcorner+nrows*idelr-idelr*(irow+1/2)\n",
    "        inode=inode+1\n",
    "    \n",
    "elevations = interpolate.griddata(truth_xy,asc_grid,cellxy,'nearest')\n",
    "\n",
    "inode=0\n",
    "for irow in range(nrow):\n",
    "    for icol in range(ncol):\n",
    "        topm[0,irow,icol]=elevations[inode]\n",
    "        inode=inode+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81046990",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spatial_data = [] # ncols, nrows, xllcorner, yllcorner, dx, dy\n",
    "\n",
    "gls_elevs = np.zeros((len(gl_files)+2, nrow, ncol), dtype=np.float32) # arreglo de elevaciones para capas guias\n",
    "                     \n",
    "gls_elevs[0,:,:] = topm[0,:,:] # se rellena para capa 0 con el topm\n",
    "gls_elevs[len(gl_files)+1,:,:] = 1800 # bottom del modelo\n",
    "\n",
    "for i,gl_file in enumerate(gl_files):\n",
    "    \n",
    "    pth = os.path.join(\"./geometrias\",gl_file)\n",
    "    with open(pth,'r') as f:\n",
    "        lines = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    asc_grid = np.loadtxt(os.path.join(\"./geometrias\",gl_file), skiprows=6)\n",
    "    \n",
    "    spatial_data = [] # ncols, nrows, xllcorner, yllcorner, dx, dy\n",
    "    \n",
    "    for j in range(6):\n",
    "        spatial_data.append(float(lines[j].split()[1]))\n",
    "\n",
    "    ncols = int(spatial_data[0])\n",
    "    nrows = int(spatial_data[1])\n",
    "    xllcorner = spatial_data[2]\n",
    "    yllcorner = spatial_data[3]\n",
    "    idelr = spatial_data[4]\n",
    "    idelc = spatial_data[4]\n",
    "     \n",
    "    asc_grid = np.reshape(asc_grid,nrows*ncols)\n",
    "    \n",
    "    truth_xy = np.empty((nrows*ncols, 2), dtype=float)\n",
    "    inode=0\n",
    "    for irow in range(nrows):\n",
    "        for icol in range(ncols):\n",
    "            truth_xy[inode,0]=xllcorner+idelc*(icol+1/2)\n",
    "            truth_xy[inode,1]=yllcorner+nrows*idelr-idelr*(irow+1/2)\n",
    "            inode=inode+1\n",
    "    \n",
    "    elevations = interpolate.griddata(truth_xy,asc_grid,cellxy,'nearest')\n",
    "    \n",
    "    inode=0\n",
    "    for irow in range(nrow):\n",
    "        for icol in range(ncol):\n",
    "            if elevations[inode]<=0:\n",
    "                gls_elevs[i+1,irow,icol]=gls_elevs[i,irow,icol]-1\n",
    "            else:\n",
    "                gls_elevs[i+1,irow,icol]=elevations[inode]\n",
    "            inode=inode+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc98d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se chequea no se cruzen capas guias\n",
    "\n",
    "for i in range(1,len(gls_elevs)):\n",
    "    for irow in range(nrow):\n",
    "        for icol in range(ncol):\n",
    "            diff = gls_elevs[i-1,irow,icol]-gls_elevs[i,irow,icol]\n",
    "            if diff <=0:\n",
    "                gls_elevs[i,irow,icol]=gls_elevs[i-1,irow,icol]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0306bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se rellena arreglo botm\n",
    "\n",
    "botm = np.zeros((nlay,nrow,ncol),dtype=float)\n",
    "\n",
    "for irow in range(nrow):\n",
    "    for icol in range(ncol):\n",
    "        ibot = gls_elevs[0,irow,icol]\n",
    "        j=0\n",
    "        for i in range(1,len(gls_elevs)):\n",
    "            delta = (gls_elevs[i-1,irow,icol]-gls_elevs[i,irow,icol])/(gls_lays[i-1])\n",
    "            for ilay in range(gls_lays[i-1]):\n",
    "                ibot = ibot-delta\n",
    "                botm[j,irow,icol]=ibot\n",
    "                j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdcc3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se chequea que no existen celdas con espesor negativo\n",
    "diff = np.zeros((nlay-1,nrow,ncol),dtype=float)\n",
    "\n",
    "for ilay in range(1,nlay):\n",
    "    for irow in range(nrow):\n",
    "        for icol in range(ncol):\n",
    "            diff[ilay-1,irow,icol] = botm[ilay-1,irow,icol]-botm[ilay,irow,icol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15884a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf62f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rellenar topm para que sea de las mismas dimensiones de bottom, se usa en alguna función\n",
    "\n",
    "topo=topm\n",
    "fila=6\n",
    "colum=10\n",
    "\n",
    "topm = np.zeros((nlay, nrow, ncol), dtype=np.float32)\n",
    "topm[0]=topo\n",
    "for i in range(nlay-1):\n",
    "    topm[i+1]=botm[i]\n",
    "    print('layer ',i, \"-celda (\", fila,\"--\",colum,\")---\", topm[i][fila][colum], botm[i][fila][colum])\n",
    "print('layer ',i+1, \"-celda (\", fila,\"--\",colum,\")---\", topm[i+1][fila][colum], botm[i+1][fila][colum])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c27574",
   "metadata": {},
   "source": [
    "# Discretización Temporal\n",
    "Se crea funcion para definir los periodos de estrés, ingresando fechas de inicio y fin, escala temporal (año, mes, día), y las otras propiedades para definición de períodos de estrés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e078c852",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def Constru_SP(year_0,month_0,day_0,year_F,month_F,day_F,escala='m',n_escala=1, steady_0=False, timestep=18, factor=2):\n",
    "    '''asdasdasdasd\n",
    "    asdasdasd'''\n",
    "    import datetime\n",
    "    \n",
    "    #Definir las listas a rellenar como resultado de la funcion\n",
    "    perlen = []\n",
    "    nstp = []\n",
    "    tsmult = []\n",
    "    steady = []\n",
    "    fecha_inicioSP=[]\n",
    "    fecha_finSP=[]\n",
    "    \n",
    "    #Se definen las fechas iniciales y finales, y las variables auxiliares correspondientes a fecha inicio y termino de SP\n",
    "    fecha_0=datetime.date(year_0,month_0,day_0)\n",
    "    fecha_F=datetime.date(year_F,month_F,day_F)\n",
    "    fecha_aux1=fecha_0\n",
    "    fecha_aux2=fecha_0\n",
    "    \n",
    "    #Se definen ciertas condiciones de ingreso datos erroneos\n",
    "    if (year_0 or year_F)>10000 and (year_0 or year_F)>1000:\n",
    "        return 'Años son mayores a 10000 o menores a 1000'\n",
    "        pass\n",
    "    \n",
    "    if (fecha_0)>=(fecha_F):\n",
    "        return 'Fecha inicio mayor que fecha final'\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "    #Se cconstruyen las listas en base a la escala usada\n",
    "    #Escala Anual 'y'\n",
    "    if escala=='y':\n",
    "        while fecha_aux2<fecha_F:\n",
    "            fecha_aux2=datetime.date(getattr(fecha_aux2,'year')+n_escala,getattr(fecha_aux2,'month'),getattr(fecha_aux2,'day'))\n",
    "            if fecha_aux2>fecha_F:\n",
    "                fecha_aux2=fecha_F            \n",
    "            perlen.append((fecha_aux2-fecha_aux1)/datetime.timedelta(days=1))\n",
    "            fecha_inicioSP.append(fecha_aux1.isoformat())\n",
    "            fecha_finSP.append(fecha_aux2.isoformat())\n",
    "            fecha_aux1=fecha_aux2\n",
    "            nstp.append(int(timestep))\n",
    "            tsmult.append(float(factor))\n",
    "            steady.append(False)\n",
    "                \n",
    "    #Escala Mensual 'm'\n",
    "    elif escala=='m':\n",
    "        while fecha_aux2<fecha_F:\n",
    "            if getattr(fecha_aux2,'month')+n_escala in [13,14,15,16,17,18,19,20,21,22,23]:\n",
    "                fecha_aux2=datetime.date(getattr(fecha_aux2,'year')+1,getattr(fecha_aux2,'month')+n_escala-12,getattr(fecha_aux2,'day'))\n",
    "                if fecha_aux2>fecha_F:\n",
    "                    fecha_aux2=fecha_F                \n",
    "                perlen.append((fecha_aux2-fecha_aux1)/datetime.timedelta(days=1))\n",
    "                fecha_inicioSP.append(fecha_aux1.isoformat())\n",
    "                fecha_finSP.append(fecha_aux2.isoformat())\n",
    "                fecha_aux1=fecha_aux2\n",
    "                nstp.append(int(timestep))\n",
    "                tsmult.append(float(factor))\n",
    "                steady.append(False)\n",
    "                \n",
    "            else:\n",
    "                fecha_aux2=datetime.date(getattr(fecha_aux2,'year'),getattr(fecha_aux2,'month')+n_escala,getattr(fecha_aux2,'day'))\n",
    "                if fecha_aux2>fecha_F:\n",
    "                    fecha_aux2=fecha_F                 \n",
    "                perlen.append((fecha_aux2-fecha_aux1)/datetime.timedelta(days=1))\n",
    "                fecha_inicioSP.append(fecha_aux1.isoformat())\n",
    "                fecha_finSP.append(fecha_aux2.isoformat())\n",
    "                fecha_aux1=fecha_aux2\n",
    "                nstp.append(int(timestep))\n",
    "                tsmult.append(float(factor))\n",
    "                steady.append(False)\n",
    "\n",
    "    #Escala Diaria 'd'            \n",
    "    elif escala=='d':\n",
    "        while fecha_aux2<fecha_F:\n",
    "            fecha_aux2+=datetime.timedelta(days=n_escala)\n",
    "            if fecha_aux2>fecha_F:\n",
    "                fecha_aux2=fecha_F\n",
    "            perlen.append((fecha_aux2-fecha_aux1)/datetime.timedelta(days=1))\n",
    "            fecha_inicioSP.append(fecha_aux1.isoformat())\n",
    "            fecha_finSP.append(fecha_aux2.isoformat())\n",
    "            fecha_aux1=fecha_aux2\n",
    "            nstp.append(int(timestep))\n",
    "            tsmult.append(float(factor))\n",
    "            steady.append(False)    \n",
    "\n",
    "    else:\n",
    "        pass\n",
    "    #Agregar si el primero SP es steady\n",
    "    steady[0] = steady_0\n",
    "    \n",
    "    #Resultado entregado por la funcion\n",
    "    return perlen, fecha_inicioSP, fecha_finSP, nstp, tsmult, steady\n",
    "\n",
    "help(Constru_SP)\n",
    "\n",
    "#Ejemplo    \n",
    "Constru_SP(2015,1,1,2021,1,1, escala='m', n_escala=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba0982e",
   "metadata": {},
   "source": [
    "Se definen periodos de estrés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928dce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "SP=Constru_SP(2015,1,1,2021,1,1, escala='m', n_escala=1)\n",
    "\n",
    "perlen = SP[0]\n",
    "fecha_inicialSP=SP[1]\n",
    "fecha_finalSP=SP[2]\n",
    "nstp = SP[3]\n",
    "tsmult = SP[4]\n",
    "steady = SP[5]\n",
    "\n",
    "nper=len(perlen)\n",
    "#Convertir fecha 'yyyy-mm-dd' a datetime\n",
    "#from datetime import datetime\n",
    "#fecha_dt=datetime.strptime(fecha_finalSP[0],'%Y-%m-%d')\n",
    "\n",
    "    \n",
    "print('largo SP \\n', perlen)\n",
    "print('\\nnúmero de time step en SP \\n', nstp)\n",
    "print('\\nfactor multi \\n', tsmult)\n",
    "print('\\nFecha inicial \\n', fecha_inicialSP)\n",
    "print('\\nFecha final \\n', fecha_finalSP)\n",
    "print('\\nn° periodos de stress \\n', nper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f68eb5",
   "metadata": {},
   "source": [
    "# Paquetes SIM y  DIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da3158",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sim = flopy.modflow.Modflow(modelname=modelname, model_ws=model_ws,\n",
    "                          version='mfusg', exe_name=mfusg_exe,\n",
    "                          structured=True)\n",
    "dis = flopy.modflow.ModflowDis(sim, nlay=nlay, nrow=nrow, ncol=ncol, delr=delr,\n",
    "                               delc=delc, top=topm[0], botm=botm,\n",
    "                              xul=xul, yul=yul, nper=nper,steady=steady,\n",
    "                               perlen=perlen,nstp=nstp,tsmult=tsmult)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7161e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = GridIntersect(sim.modelgrid, method=\"vertex\", rtree=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6db965",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row=15\n",
    "col=5\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "# Next we create an instance of the ModelCrossSection class\n",
    "#modelxsect = flopy.plot.ModelCrossSection(model=ml, line={'Column': col})\n",
    "modelxsect = flopy.plot.PlotCrossSection(model=sim, line={'Row':row})\n",
    "\n",
    "# Then we can use the plot_grid() method to draw the grid\n",
    "# The return value for this function is a matplotlib LineCollection object,\n",
    "# which could be manipulated (or used) later if necessary.\n",
    "linecollection = modelxsect.plot_grid(linewidth=0.8)\n",
    "t = ax.set_title('Row {} Cross-Section - Model Grid'.format(row))\n",
    "ax.set_ylim([1800,2400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9000e0e7",
   "metadata": {},
   "source": [
    "# Evaluacion geologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d63108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Id,X,Y,Z,dX,dY,dZ,HM_FinalModel on 02-06-2021\n",
    "\n",
    "geol_unit_file = './eval_geologia/BM_HM_500x500x10.csv'\n",
    "\n",
    "geol_units = pd.read_csv(geol_unit_file,\n",
    "           header=None, names=['Id','X','Y','Z','dX','dY','dZ','GU'],\n",
    "           skipinitialspace=True,skiprows=11)\n",
    "\n",
    "geol_units.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93979498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elevaciones modelo geologico\n",
    "geol_elev = geol_units['Z'].unique()[::-1]\n",
    "\n",
    "#Coordenadas modelo\n",
    "model_x = sim.modelgrid.xcellcenters\n",
    "model_y =sim.modelgrid.ycellcenters\n",
    "model_z =sim.modelgrid.zcellcenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9274205",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "geol_eval = np.zeros((nlay,nrow,ncol))\n",
    "\n",
    "for irow in range(nrow):\n",
    "    for icol in range(ncol):\n",
    "        index = 0\n",
    "        z_geo_select = 0\n",
    "        for ilay in range(nlay):\n",
    "            elev_diff = 10000\n",
    "            z = model_z[ilay,irow,icol]\n",
    "            for i in range(len(geol_elev)):\n",
    "                ielev_diff=abs(z-geol_elev[i])\n",
    "                if ielev_diff < elev_diff:\n",
    "                    index = i\n",
    "                    elev_diff = ielev_diff\n",
    "            z_geo_select = geol_elev[index]\n",
    "            df_elev_select = geol_units[geol_units['Z']==z_geo_select].values\n",
    "            \n",
    "            geol_xy = np.empty((len(df_elev_select), 2), dtype=float)\n",
    "            \n",
    "            for i in range(len(df_elev_select)):\n",
    "                geol_xy[i][0]=df_elev_select[i][1]\n",
    "                geol_xy[i][1]=df_elev_select[i][2]\n",
    "            \n",
    "            igeol = df_elev_select[:,7]\n",
    "            \n",
    "            ivalue = interpolate.griddata(geol_xy,igeol,(model_x[irow,icol],model_y[irow,icol]),'nearest')\n",
    "            \n",
    "            geol_eval[ilay,irow,icol]=ivalue\n",
    "            \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d16747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(15,15))\n",
    "\n",
    "figure1 = ax.imshow(geol_eval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04888625",
   "metadata": {},
   "outputs": [],
   "source": [
    "row=15\n",
    "\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "# Next we create an instance of the ModelCrossSection class\n",
    "#modelxsect = flopy.plot.ModelCrossSection(model=ml, line={'Column': 5})\n",
    "modelxsect = flopy.plot.PlotCrossSection(model=sim, line={'Row': row})\n",
    "\n",
    "# Then we can use the plot_grid() method to draw the grid\n",
    "# The return value for this function is a matplotlib LineCollection object,\n",
    "# which could be manipulated (or used) later if necessary.\n",
    "linecollection = modelxsect.plot_grid(linewidth=0.8)\n",
    "t = ax.set_title('Row {} Cross-Section - Model Grid'.format(row))\n",
    "\n",
    "pc = modelxsect.plot_array(geol_eval,alpha=0.8)\n",
    "\n",
    "ax.set_ylim([1800,2400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647b0a7a",
   "metadata": {},
   "source": [
    "# Definir Celdas Activas\n",
    "Crear Noflow desde shape que tenga la zona activa (el shape debe tener si es activo o no (1 o 0), y el layer de cada shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980c3c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_cell(sim,shp_noflow_obj,nlay,nrow,ncol):\n",
    "    \n",
    "    # Se inicializa la GridIntersect\n",
    "    ix = GridIntersect(sim.modelgrid, method=\"vertex\", rtree=True)\n",
    "    \n",
    "    noflow = np.zeros((nlay,nrow,ncol), dtype=int)\n",
    "    \n",
    "    shapeRecs = shp_noflow_obj.shapeRecords()  \n",
    "    \n",
    "    for ishape in shapeRecs:\n",
    "        if int(ishape.record['active'])==1:\n",
    "            shape = ishape.shape\n",
    "            ilayer=ishape.record['layer']-1 #reemplazar si cada layer tiene 1 shape\n",
    "            p = Polygon(shell=shape.points)\n",
    "            result = ix.intersect(p)['cellids']\n",
    "            for icell in range(len(result)):\n",
    "                noflow[ilayer,result[icell][0],result[icell][1]]=1  #reemplazar 0 por ilayer si cada layer tiene 1 shape\n",
    "                \n",
    "    return noflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar shape\n",
    "shp_noflow_obj = shp.Reader('./geometria/Activo-NoActivo_dissolve.shp')\n",
    "\n",
    "# Celdas activas\n",
    "noflow = active_cell(sim,shp_noflow_obj,nlay,nrow,ncol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eced1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "noflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5632294c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(15,15))\n",
    "\n",
    "figure1 = ax1.imshow(noflow[0])\n",
    "figure2 = ax2.imshow(noflow[1])\n",
    "figure3 = ax3.imshow(noflow[2])\n",
    "\n",
    "ax1.set_xlabel('Columns'); ax1.set_ylabel('Rows')\n",
    "ax2.set_xlabel('Columns'); ax2.set_ylabel('Rows')\n",
    "ax3.set_xlabel('Columns'); ax3.set_ylabel('Rows')\n",
    "\n",
    "ax1.title.set_text('Dominio Capa 1')\n",
    "ax2.title.set_text('Dominio Capa 2')\n",
    "ax3.title.set_text('Dominio Capa 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91378fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corroborar que top se encuentre sobre bottom de cada layer de las celdas activas\n",
    "\n",
    "\n",
    "for i in range(nlay):\n",
    "    Dif_minima=0\n",
    "    celda_min=0\n",
    "    for j in range(nrow):\n",
    "        for k in range(ncol):\n",
    "            if noflow[i,j,k]==0:\n",
    "                pass\n",
    "            elif Dif_minima>(topm[i][j][k]-botm[i][j][k]):\n",
    "                Dif_minima=topm[i][j][k]-botm[i][j][k]\n",
    "                celda_min=[j,k]\n",
    "    print('Layer ', i, '- Celda ', celda_min, '- Diferencia max ', Dif_minima)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc91f89",
   "metadata": {},
   "source": [
    "# Se crea el paquete BAS (starting head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67b3cb1",
   "metadata": {},
   "source": [
    "Se cargan los raster relacionados a las piezometrías para las distintas unidades, y los layer definidos para cada una de estas (linker si se definen antes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8c7819",
   "metadata": {},
   "outputs": [],
   "source": [
    "bas = flopy.modflow.ModflowBas(sim, strt= 100,ibound=noflow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aed0da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "starthead = np.ones((nlay,nrow,ncol), dtype=float)*2300\n",
    "starh_UA= Raster.load('./starting_head/Mod_Nucleo_Piezo_UA_2015.asc')\n",
    "starh_UB= Raster.load('./starting_head/Mod_Nucleo_Piezo_UB_2015.asc')\n",
    "starh_UC= Raster.load('./starting_head/Mod_Nucleo_Piezo_UC_2015.asc')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4165940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_starthead(starthead,sim,starh,lim_top,lim_bottom):\n",
    "    dem_data = starh.resample_to_grid(\n",
    "    sim.modelgrid, band=starh.bands[0], method=\"nearest\", extrapolate_edges=True)\n",
    "    dem_data_np = pd.DataFrame(dem_data).replace(pd.DataFrame(dem_data).iloc[0][0],2300*1).to_numpy() #dem_data to numpy\n",
    "    \n",
    "    #escribir el nivel inicial en la unidad respectiva\n",
    "    starthead[(lim_top-1):(lim_bottom-1)] = dem_data_np\n",
    "    \n",
    "    return starthead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781e8df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting Head UA\n",
    "starthead = fun_starthead(starthead,sim,starh_UA,layerUA_top,layerUB_top)\n",
    "# Starting Head UB\n",
    "starthead = fun_starthead(starthead,sim,starh_UB,layerUB_top,layerUC_top)\n",
    "# Starting Head UC\n",
    "starthead = fun_starthead(starthead,sim,starh_UC,layerUC_top,nlay+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9892beef",
   "metadata": {},
   "outputs": [],
   "source": [
    "starthead[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e607abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = starh_UA.get_array(1)\n",
    "idx = np.isfinite(arr)\n",
    "\n",
    "vmin, vmax = arr[idx].min(), arr[idx].max()\n",
    "vmin, vmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e512e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lay_starthead=0\n",
    "# now to visualize using flopy and matplotlib\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(1, 1, 1, aspect=\"equal\")\n",
    "modelmap = flopy.plot.map.PlotMapView(model=sim)\n",
    "\n",
    "pmv = flopy.plot.PlotMapView(modelgrid=sim.modelgrid, ax=ax)\n",
    "ax = pmv.plot_array(\n",
    "    starthead[lay_starthead], masked_values=starh_UA.nodatavals, vmin=vmin, vmax=vmax\n",
    ")\n",
    "quadmesh = modelmap.plot_ibound(color_noflow='cyan') #celdas inactivas en celeste\n",
    "plt.colorbar(ax, shrink=0.7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e646e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "strt = starthead\n",
    "\n",
    "bas = flopy.modflow.ModflowBas(sim, strt= strt,ibound=noflow)\n",
    "bas.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd1a9ca",
   "metadata": {},
   "source": [
    "# Se crean los paquetes Lpf, Sms, Oc\n",
    "Para el paquete RCH y ETS se programa su creacion. La secuencia es que primero se crean los paquetes con FLOPY Luego se agregan los paquetes RCH y ETS, actualizandose el .nam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e06ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpf = flopy.modflow.ModflowLpf(sim, hk=500, ipakcb=50, laytyp=4, layvka=1, constantcv=1, novfc=1)\n",
    "lpf.write_file()\n",
    "\n",
    "\n",
    "oc = flopy.modflow.ModflowOc(sim, \n",
    "                             stress_period_data={(0,0): ['print budget', 'print head', \n",
    "                                                         'save head']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa769b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms = flopy.modflow.ModflowSms(sim,hclose=0.0001,hiclose=0.001,\n",
    "                              mxiter=2000,iter1=20,iprsms=1,\n",
    "                              nonlinmeth=1,linmeth=1,\n",
    "                              theta=0.7, akappa=0.1, gamma=0.02,\n",
    "                              amomentum=0.01,numtrack=10,btol=1.5,\n",
    "                              breduc=0.2,reslim=100,clin=\"\",\n",
    "                              ipc=1,iscl=2,iord=7,rclosepcgu=0.97)\n",
    "sms.write_file()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782608fb",
   "metadata": {},
   "source": [
    "# Reconfiguración Paquete LPF\n",
    "Se actualiza paquete lpf para que referencie archivos externos OPEN/CLOSE, definidos en el modelo original E108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fbefbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_ws,modelname+\".lpf\"),'r') as f:\n",
    "    lines = f.readlines()\n",
    "f.close()\n",
    "with open(os.path.join(model_ws,modelname+\".lpf\"),'w') as g:\n",
    "    for line in range(len(lines)):\n",
    "        g.write(lines[line])\n",
    "        if line == 6:\n",
    "            break\n",
    "#Se tiene código para escribir los parámetros según la cantidad de layer y la línea del parámetro\n",
    "#    paraLPF6=' 0'\n",
    "#    g.write(''.join([paraLPF6 for i in range(nlay)])+'\\n')\n",
    "    for i in range(nlay):\n",
    "        g.write(\"OPEN/CLOSE \"+\"1280\"+str(i+1)+\"._kx 1.000000e+00 (FREE) -1 Kx Layer \"+str(i+1)+\"\\n\")\n",
    "        g.write(\"OPEN/CLOSE \"+\"1280\"+str(i+1)+\"._kz 1.000000e+00 (FREE) -1 Kx/Kz Layer \"+str(i+1)+\"\\n\")\n",
    "        g.write(\"OPEN/CLOSE \"+\"1280\"+str(i+1)+\"._s1 1.000000e+00 (FREE) -1 Ss Layer \"+str(i+1)+\"\\n\")\n",
    "        g.write(\"OPEN/CLOSE \"+\"1280\"+str(i+1)+\"._s2 1.000000e+00 (FREE) -1 Sy Layer \"+str(i+1)+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e0b331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#se carga el paquete lpf según lo obtenido de los archivos 1280.xxx\n",
    "lpf = flopy.modflow.ModflowLpf.load('mnsda/mnsda.lpf',sim)\n",
    "ax = lpf.hk.plot(colorbar=True, grid=True)\n",
    "#ejemplo lectura de una celda en particular\n",
    "lpf.hk.array[0][24][0]  #[layer][row][column]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2925e14",
   "metadata": {},
   "source": [
    "# CB Recarga Lateral\n",
    "Se cargan las condiciones de borde de flujo lateral desde shapefile y se definen estas CBs en el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b37da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bc_rch(shp_reclat_obj,sim,criterio_K,noflow,delr,perc=0.15):\n",
    "    shp_reclat_rec = shp_reclat_obj.records()\n",
    "    shp_reclat_shapes = shp_reclat_obj.shapes()\n",
    "\n",
    "    shapeRecs = shp_reclat_obj.shapeRecords()   \n",
    "    ix = GridIntersect(sim.modelgrid, method=\"vertex\", rtree=True)\n",
    "\n",
    "    #Se inician variables que guardaran los nodos de RecLat, caudal, id recarga, Layer\n",
    "    nodos_reclat = []\n",
    "    qin_reclat = []\n",
    "    rec_reclat = []\n",
    "    lay_reclat = []\n",
    "\n",
    "    #se recorre cada shape asociado a la polilínea de recarga lateral\n",
    "    for shaperec in shapeRecs:\n",
    "        celdas_reclat = []\n",
    "        nrec = 0\n",
    "\n",
    "        #Se guarda el id de recarga y layer para cada polilínea con la recarga lateral\n",
    "        rec = shaperec.record['id_recarga']\n",
    "        from_layer = shaperec.record['from_layer']\n",
    "        to_layer = shaperec.record['to_layer']\n",
    "\n",
    "        #Se intersecta la polilínea con la grilla, guardando los id de la celda y el largo de intersección\n",
    "        pline = shaperec.shape.points\n",
    "        ls = LineString(pline)\n",
    "        cells = ix.intersect(ls)['cellids']\n",
    "        largo_inter = ix.intersect(ls)['lengths']\n",
    "\n",
    "        # Se recorre cada celda intersectada por la polilínea, según los layer en los cuales está la CB\n",
    "        #para guardar cada una de ellas\n",
    "        for k in range(from_layer,to_layer+1):\n",
    "            for id,icell in enumerate(cells):\n",
    "                # No se consideran celdas que se encuentran inactivas\n",
    "                if noflow[k-1,icell[0],icell[1]] == 0:\n",
    "                    pass\n",
    "                # No se consideran celdas que la intersección sea menor al 'perc' % del largo de la celda\n",
    "                elif largo_inter[id]<perc*delr:\n",
    "                    pass\n",
    "                elif lpf.hk.array[k-1][icell[0]][icell[1]] < criterio_K:\n",
    "                    pass\n",
    "                else:\n",
    "                    #Se guarda el id de recarga, la celda, y el layer de la celda intersectada\n",
    "                    rec_reclat.append(rec)\n",
    "                    celdas_reclat.append(icell)\n",
    "                    lay_reclat.append(k)\n",
    "\n",
    "        nrec = len(celdas_reclat)\n",
    "\n",
    "        # se divide el caudal total de la polilínea por la cantidad de celdas intersectadas\n",
    "        iq_rl = q_rl2[q_rl2['id_recarga'] == rec]['Q(m3/d)'].tolist()\n",
    "        iq_rl=iq_rl[0]/nrec\n",
    "\n",
    "        #Se asocia un caudal a cada celda intersectada\n",
    "        for icell in celdas_reclat:\n",
    "            nodos_reclat.append(icell)\n",
    "            qin_reclat.append(iq_rl)\n",
    "\n",
    "\n",
    "    wel_dtype = [('k', '<i8'), ('i', '<i8'), ('j', '<i8'), \n",
    "                ('flux', '<f4'), ('iwel', '<i4')]\n",
    "    wel_dtype = np.dtype(wel_dtype)\n",
    "\n",
    "    sp1 = np.zeros((len(nodos_reclat)), dtype=wel_dtype)\n",
    "    sp1 = sp1.view(np.recarray)\n",
    "\n",
    "    # se leen las celdas, y se asigna una variable auxiliar asociada con la zona de recarga\n",
    "    for icell in range(len(nodos_reclat)):\n",
    "        sp1[icell] = (lay_reclat[icell]-1,nodos_reclat[icell][0],nodos_reclat[icell][1]\n",
    "                    ,qin_reclat[icell],rec_reclat[icell]+100)\n",
    "\n",
    "\n",
    "    # Ahora se repite la operacion pero para el SP2, tal de generar la variable auxiliar que\n",
    "    # diferencia esta condicion de borde del SP1. El valor de variable auxiliar es 200+Z donde\n",
    "    # es el numero de zona identificado\n",
    "\n",
    "    sp2 = np.zeros((len(nodos_reclat)), dtype=wel_dtype)\n",
    "    sp2 = sp2.view(np.recarray)\n",
    "\n",
    "    for icell in range(len(nodos_reclat)):\n",
    "        sp2[icell] = (lay_reclat[icell]-1,nodos_reclat[icell][0],nodos_reclat[icell][1]\n",
    "                    ,qin_reclat[icell],rec_reclat[icell]+200)\n",
    "\n",
    "    return sp1,sp2,nodos_reclat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1870cbd",
   "metadata": {},
   "source": [
    "Se intersecta el polígono de las líneas con las condiciones de borde de recarga con la grilla (shape con polilíneas con el layer de la CB y el id de la recarga, que se lee del csv), donde a las celdas intersectadas se les asigna un caudal según la tabla con los valores de recarga lateral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a09732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se leen las polilineas\n",
    "shp_reclat_obj = shp.Reader(\"./recarga_lateral/recarga_lateral.shp\")\n",
    "\n",
    "#Se cargan los caudales desde archivo\n",
    "q_rl2 = pd.read_csv(\"./recarga_lateral/Recarga_Lateral_Q.csv\")\n",
    "\n",
    "criterio_K = 0.01\n",
    "\n",
    "sp1,sp2,nodos_reclat = bc_rch(shp_reclat_obj,sim,criterio_K,noflow,delr)\n",
    "\n",
    "stress_period_data = {0:sp1,1:sp2}\n",
    "\n",
    "wel_dtype = [('k', '<i8'), ('i', '<i8'), ('j', '<i8'), \n",
    "                ('flux', '<f4'), ('iwel', '<i4')]\n",
    "wel_dtype = np.dtype(wel_dtype)\n",
    "\n",
    "wel1 = flopy.modflow.ModflowWel(sim, ipakcb=50,stress_period_data=stress_period_data,\n",
    "                                dtype=wel_dtype,options=['AUTOFLOWREDUCE','aux iwel'])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "sim.modelgrid.plot(ax=ax)\n",
    "\n",
    "for irow, icol in nodos_reclat:\n",
    "    h2, = ax.plot(sim.modelgrid.xcellcenters[0, icol], sim.modelgrid.ycellcenters[irow, 0],\n",
    "                  \"kx\", label=\"centroids of intersected gridcells\")\n",
    "\n",
    "ax.legend([h2], [i.get_label() for i in [h2]], loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a130b7e1",
   "metadata": {},
   "source": [
    "# Bombeo-Extracciones\n",
    "Se lee el archivo con los datos de pozos de bombeo para crear paquete WELL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se lee archivo de ubicacioan de pozos\n",
    "\n",
    "wells_ubi = pd.read_csv(\"./well/Ubicacion_Well_all.csv\")\n",
    "wells_ubi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca284499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se lee archivo de bombeo\n",
    "\n",
    "wells = pd.read_csv(\"./well/Serie_Well_all.csv\")\n",
    "wells.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24ae9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_to_day(df):\n",
    "    '''Convierte del DataFrame mensual en un caudal uniforme diario\n",
    "    retornando un diccionario con los caudales diarios de todos los pozos'''\n",
    "    \n",
    "    dic_well = {} #diccionario vacio donde se almacenará el DataFrame de cada pozo\n",
    "    name_well = df['pozo'].unique() #nombre de los pozos\n",
    "    num_well = len(df['pozo'].unique()) #cantidad de pozos\n",
    "\n",
    "    for i in range(num_well): #recorrer los pozos\n",
    "        df_well = df[df['pozo'] == name_well[i]]\n",
    "        df_well.reset_index(drop=True,inplace=True)  \n",
    "\n",
    "        df_new = pd.DataFrame(columns = ['Days','Flow']) #DataFrame a rellenar\n",
    "\n",
    "        for j in range(len(df_well['Fecha_inicio'])): #recorrer meses de cada pozo\n",
    "            date_i = datetime.datetime.strptime(df_well['Fecha_inicio'][j], \"%d/%m/%Y\")\n",
    "            date_f = datetime.datetime.strptime(df_well['Fecha_fin'][j], \"%d/%m/%Y\")\n",
    "            delta = date_f - date_i\n",
    "\n",
    "            days = []\n",
    "            for k in range(delta.days): #agregar dias del mes\n",
    "                days.append(date_i + datetime.timedelta(days=k))\n",
    "\n",
    "            #almecenar datos\n",
    "            df_aux = pd.DataFrame()\n",
    "            df_aux['Days'] = days\n",
    "            df_aux['Flow'] = np.repeat(df_well['Caudal_m3/d'][j],len(days))\n",
    "\n",
    "            #concaquetar datos\n",
    "            df_new = pd.concat([df_new,df_aux],axis=0)\n",
    "\n",
    "        #guardar datos en diccionario\n",
    "        df_new.reset_index(drop=True,inplace=True)\n",
    "        dic_well[name_well[i]] = df_new\n",
    "    \n",
    "    return dic_well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bb0225",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_well_day = well_to_day(wells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05600437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_to_SP(SP,dic_well):\n",
    "    \n",
    "    #Data del SP\n",
    "    fecha_inicialSP=SP[1]\n",
    "    fecha_finalSP=SP[2]\n",
    "    \n",
    "    name_well = list(dic_well.keys())\n",
    "    \n",
    "    df_sp = pd.DataFrame(columns = ['pozo','sp','Fecha_inicio','Fecha_fin','Caudal']) #DataFrame a rellenar\n",
    "    \n",
    "    for well in name_well: #recorrer pozos\n",
    "        df_well = dic_well[well]\n",
    "        Caudal = []\n",
    "        sp = []\n",
    "        \n",
    "        for i in range(len(fecha_inicialSP)):\n",
    "            sp.append(i+1)\n",
    "            \n",
    "            #Filtrar segun fechas\n",
    "            f_i = df_well['Days'] >= fecha_inicialSP[i]\n",
    "            f_f = df_well['Days'] < fecha_finalSP[i]\n",
    "            filt = f_i & f_f\n",
    "            Caudal.append(np.mean(df_well.loc[filt]['Flow'])) #Caudal promedio segun fechas establecidas\n",
    "            \n",
    "        dic = {'pozo':[well]*len(sp),'sp':sp,'Fecha_inicio':fecha_inicialSP,'Fecha_fin':fecha_finalSP,'Caudal':Caudal}         \n",
    "        df_sp_well = pd.DataFrame(dic) #DataFrame de un pozo\n",
    "        \n",
    "        df_sp = pd.concat([df_sp,df_sp_well],axis=0) \n",
    "        \n",
    "\n",
    "    df_sp.dropna(subset = [\"Caudal\"], inplace=True) #Eliminar filas NaN\n",
    "    df_sp.reset_index(drop=True,inplace=True)       #Reiniciar index\n",
    "    \n",
    "    return df_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b14d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_well_to_sp = well_to_SP(SP,dic_well_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28651945",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_well_to_sp.loc[df_well_to_sp.pozo=='ARPES-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ec100",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_well_to_sp.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037d62be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_layer(wells_ubi,sim,ALB_bot_well,topm,botm,nlay):\n",
    "\n",
    "    ix = GridIntersect(sim.modelgrid, method=\"vertex\", rtree=True)\n",
    "\n",
    "    #asignar los layer a cada uno de los pozos, van desde 1 a nlay\n",
    "    top_lay_well = np.ones(len(wells_ubi['pozo']),dtype='int')\n",
    "    bot_lay_well = np.ones(len(wells_ubi['pozo']),dtype='int')\n",
    "\n",
    "    #se recorre cada pozo, guardando ubicacion, compañia \n",
    "    for n,pozo in enumerate(wells_ubi['pozo']):\n",
    "        x = wells_ubi.loc[n,'X']\n",
    "        y = wells_ubi.loc[n,'Y']\n",
    "        comp = wells_ubi.loc[n,'Comp']\n",
    "        pt = Point(x,y)\n",
    "        cells = ix.intersect(pt)['cellids'] \n",
    "        \n",
    "        #Se asigna a los pozos de ALB un layer por defecto, variable de asignación previa\n",
    "        if comp == 'ALB':\n",
    "            bot_lay_well[n] = ALB_bot_well\n",
    "            top_lay_well[n] = 1    \n",
    "        else: \n",
    "            #Se asignan valores de layer si es que queda sobre la grilla del modelo o bajo \n",
    "            if (wells_ubi.loc[n,'Bot_scr'] <= (botm[nlay-1][cells[0]])):\n",
    "                bot_lay_well[n] = nlay\n",
    "        \n",
    "            if (wells_ubi.loc[n,'Top_scr'] >= (topm[0][cells[0]])):\n",
    "                top_lay_well[n] = 1\n",
    "\n",
    "            #Se recorre cada bottom layer desde abajo hacia arriba \n",
    "            for i in range(nlay,0,-1):\n",
    "                if (wells_ubi.loc[n,'Bot_scr'] >= (botm[i-1][cells[0]])) & (wells_ubi.loc[n,'Bot_scr'] <= (topm[i-1][cells[0]])):\n",
    "                    bot_lay_well[n] = i\n",
    "                    break\n",
    "                \n",
    "            for i in range(nlay,0,-1):\n",
    "                if (wells_ubi.loc[n,'Top_scr'] >= (botm[i-1][cells[0]])) & (wells_ubi.loc[n,'Top_scr'] <= (topm[i-1][cells[0]])):\n",
    "                    top_lay_well[n] = i\n",
    "                    break\n",
    "\n",
    "    wells_ubi['top_layer'] = top_lay_well          \n",
    "    wells_ubi['bot_layer'] = bot_lay_well\n",
    "\n",
    "    return wells_ubi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a67e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALB_bot_well = 5\n",
    "wells_ubi = assign_layer(wells_ubi,sim,ALB_bot_well,topm,botm,nlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ae9749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corroborar que el top de todos lo pozos se encuentra sobre el bot de los pozos\n",
    "(wells_ubi.top_layer-wells_ubi.bot_layer).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceb42c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dic_sp_well(df_well_to_sp,nper,sim):\n",
    "\n",
    "    # Empty dic \n",
    "    stress_period_data = {}\n",
    "\n",
    "    wel_dtype = [('k', '<i8'), ('i', '<i8'), ('j', '<i8'), \n",
    "              ('flux', '<f4'), ('iwel', '<i4')]\n",
    "    wel_dtype = np.dtype(wel_dtype)\n",
    "\n",
    "    ix = GridIntersect(sim.modelgrid, method=\"vertex\", rtree=True)\n",
    "\n",
    "    # Ir agregando info de pozos por SP\n",
    "    for iper in range(nper):\n",
    "    \n",
    "        wells_sp = df_well_to_sp[df_well_to_sp[\"sp\"]==iper+1]\n",
    "        name = wells_sp['pozo'].tolist()\n",
    "        x = wells_ubi.loc[wells_ubi.pozo.isin(name),'X'].tolist()\n",
    "        y = wells_ubi.loc[wells_ubi.pozo.isin(name),'Y'].tolist()\n",
    "        xy = list(zip(x, y))\n",
    "        lay = np.ones(len(x))  #auxiliar por si no funciona\n",
    "        q = wells_sp['Caudal'].tolist()\n",
    "\n",
    "        sp = np.zeros((len(x)), dtype=wel_dtype)\n",
    "        sp = sp.view(np.recarray)\n",
    "\n",
    "    #incorporar al archivo SP cada uno de los bombeos\n",
    "        for i in range(len(wells_sp)):\n",
    "            pt = Point(x[i],y[i])\n",
    "            cells = ix.intersect(pt)['cellids']\n",
    "\n",
    "            #asignar al layer inferior el caudal del pozo\n",
    "            lay = wells_ubi.loc[wells_ubi.pozo==name[i]].bot_layer\n",
    "        \n",
    "            #asignar la variable auxiliar\n",
    "            if q[i]>0:\n",
    "                auxvar = 50\n",
    "            else:\n",
    "                auxvar = 55\n",
    "            \n",
    "            #si va en 1 o más layert, distribuir el caudal\n",
    "            sp[i] = (lay-1,cells[0][0],cells[0][1],q[i],auxvar)\n",
    "        \n",
    "        stress_period_data_sp = {iper:sp}\n",
    "        stress_period_data.update(stress_period_data_sp)\n",
    "        \n",
    "    return stress_period_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c81562",
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_period_data =  dic_sp_well(df_well_to_sp,nper,sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923fd702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wel2 = flopy.modflow.ModflowWel(sim, ipakcb=50,stress_period_data=stress_period_data,\n",
    "                                dtype=wel_dtype,options=['AUTOFLOWREDUCE','aux iwel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85774520",
   "metadata": {},
   "source": [
    "Ahora se combinan los paquetes wel de recarga lateral y de bombeo/reinyeccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a9d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "wel3 = flopy.modflow.ModflowWel(sim,ipakcb=50,stress_period_data=\\\n",
    "                                wel2.stress_period_data.append(\n",
    "                                        wel1.stress_period_data),options=['AUTOFLOWREDUCE','aux iwel'],\n",
    "                               dtype=wel_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b48160c",
   "metadata": {},
   "source": [
    "# Pre Escribir los paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951bdf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.write_input()\n",
    "#en el caso del lpf lee los archivo 1280.xxx y los adopta con formato lpf original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6759325b",
   "metadata": {},
   "source": [
    "# Modificar y escribir paquete SMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188737e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms = flopy.modflow.ModflowSms(sim,hclose=0.00001,hiclose=0.00001,\n",
    "                              mxiter=15000,iter1=25,iprsms=2,\n",
    "                              nonlinmeth=1,linmeth=2,\n",
    "                              theta=0.7, akappa=0.07, gamma=0.01,\n",
    "                              amomentum=0.005,numtrack=15,btol=10,\n",
    "                              breduc=0.2,reslim=1,clin=\"BCGS\",\n",
    "                              ipc=1,iscl=2,iord=0,rclosepcgu=0.97)\n",
    "sms.write_file()\n",
    "\n",
    "with open(os.path.join(model_ws,modelname+\".sms\"),'r') as f:\n",
    "    lines = f.readlines()\n",
    "    print(lines)\n",
    "    print(len(lines))\n",
    "\n",
    "f.close()\n",
    "\n",
    "lines[1]=lines[1][:-1]+' SOLVEACTIVE DAMPBOT\\n'\n",
    "lines[3]=lines[3][:-5]\n",
    "#lines[3]='1 2 7 14 0 0.0 1 0.001 \\n'\n",
    "print(lines)\n",
    "\n",
    "with open(os.path.join(model_ws,modelname+\".sms\"),'w') as g:\n",
    "    g.writelines(lines)\n",
    "\n",
    "g.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d40811",
   "metadata": {},
   "source": [
    "# Escribir Paquete OC\n",
    "Se re-crea el paquete OC con configuración ATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b11104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "oc.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468e8e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(os.path.join(model_ws,modelname+\".oc\"),\"w\") as f:\n",
    "    f.write(' ATSA NPSTPS 1000\\n')\n",
    "    f.write('  HEAD SAVE UNIT 51\\n')\n",
    "    f.write('  HEAD PRINT FORMAT 0\\n')\n",
    "    #f.write('  DRAWDOWN SAVE UNIT 31\\n')\n",
    "    #f.write('  DRAWDOWN PRINT FORMAT 0\\n')\n",
    "    f.write('  CONC SAVE UNIT 132\\n')\n",
    "    f.write('  CONC PRINT FORMAT 0 \\n')\n",
    "    \n",
    "    for iper in range(nper):\n",
    "        f.write('PERIOD '+str(iper+1)+'\\n')\n",
    "        f.write('   DELTAT 1.000000e+00\\n')\n",
    "        f.write('   TMINAT 1.000000e-01\\n')\n",
    "        f.write('   TMAXAT 2.000000e+02\\n')\n",
    "        f.write('   TADJAT 2.000000e+00\\n')\n",
    "        f.write('   TCUTAT 2.000000e+00\\n')\n",
    "        f.write('   SAVE HEAD\\n')\n",
    "        #f.write('   SAVE DRAWDOWN\\n')\n",
    "        f.write('   SAVE BUDGET\\n')\n",
    "        f.write('   PRINT BUDGET\\n')\n",
    "        f.write('  SAVE CONC\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cdc306",
   "metadata": {},
   "source": [
    "# Escribir RCH \n",
    "importar pozas y zonas de recarga natural al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ccdc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar funciones creadas por AE para las recargas naturales y pozas\n",
    "import Mod_utils_RCH_AE as Mod_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9968ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recarga natural\n",
    "filename = \"./recarga_directa/zonas_recarga_model_AE.shp\"\n",
    "zone_array = np.zeros((1,nrow,ncol))\n",
    "recarga = Mod_utils.zonas_pozas(sim,filename,zone_array,1)\n",
    "zonas_recarga = recarga[0]  #tengo la zona en cada nodo\n",
    "areas_recarga = recarga[1]\n",
    "areas_recarga.shape\n",
    "#n_areas_rec = areas_recarga.shape[0] #num de areas recarga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc3346",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recarga pozas\n",
    "\n",
    "#filename = \"C:/Users/Alvespinoza/Documents/Alvaro/Incertidumbre Tomas/Seba_M/ModelConstruction/recarga_directa/zonas_recarga_model.shp\"\n",
    "#recarga2 = Mod_utils.zonas_pozas(sim,filename,recarga[0])\n",
    "\n",
    "filename_pozas = \"./recarga_directa/Pozas.shp\"\n",
    "zone_array_p = np.zeros((1,nrow,ncol),dtype=float)\n",
    "recarga_pozas = Mod_utils.zonas_pozas(sim,filename_pozas,zone_array_p,3)\n",
    "\n",
    "zonas_recarga_pozas = recarga_pozas[0] #tengo la zona en cada nodo\n",
    "areas_recarga_pozas = recarga_pozas[1]\n",
    "N_areas_pozas=areas_recarga_pozas.shape[0]\n",
    "#areas_recarga_pozas[407][20][12]   #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e791d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recarga acopio MOP\n",
    "\n",
    "filename_acopio_m = \"./recarga_directa/Acopio_MOP.shp\"\n",
    "zone_array_am = np.zeros((1,nrow,ncol),dtype=float)\n",
    "recarga_acopio_m = Mod_utils.zonas_pozas(sim,filename_acopio_m,zone_array_am,3)\n",
    "\n",
    "areas_recarga_acopio_m = recarga_acopio_m[1]\n",
    "\n",
    "#recarga acopio SOP\n",
    "\n",
    "filename_acopio_s = \"./recarga_directa/Acopio_SOP.shp\"\n",
    "zone_array_as = np.zeros((1,nrow,ncol),dtype=float)\n",
    "recarga_acopio_s = Mod_utils.zonas_pozas(sim,filename_acopio_s,zone_array_as,3)\n",
    "\n",
    "areas_recarga_acopio_s = recarga_acopio_s[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a780ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas_recarga_acopio_s[0][14][18] #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c505b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Areglo con el area total de las pozas por cada nodo, y otros 2 con el area total de cada acopio en cada nodo\n",
    "\n",
    "areas_arr_s = np.zeros((1,nrow,ncol), dtype=float)\n",
    "areas_arr_mop = np.zeros((1,nrow,ncol), dtype=float)\n",
    "areas_arr_sop = np.zeros((1,nrow,ncol), dtype=float)\n",
    "for irow in range(nrow):\n",
    "    for icol in range(ncol):\n",
    "        iarea = 0\n",
    "        imop = 0\n",
    "        isop = 0\n",
    "        for ipol in range(areas_recarga_pozas.shape[0]):\n",
    "            iarea = iarea + areas_recarga_pozas[ipol,irow,icol]\n",
    "        for imo in range(areas_recarga_acopio_m.shape[0]):\n",
    "            imop = imop + areas_recarga_acopio_m[imo,irow,icol]\n",
    "        for iso in range(areas_recarga_acopio_s.shape[0]):\n",
    "            isop = isop + areas_recarga_acopio_s[iso,irow,icol]\n",
    "        \n",
    "        areas_arr_s[0,irow,icol]=iarea\n",
    "        areas_arr_mop[0,irow,icol]=imop\n",
    "        areas_arr_sop[0,irow,icol]=isop\n",
    "\n",
    "areas_arr_sop.shape\n",
    "\n",
    "#fig, ax = plt.subplots(1,1,figsize=(15,15))\n",
    "#figure1 = ax.imshow(areas_arr_mop[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146e7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Area_nodo = delc * delr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa9542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASA RECARGA NATURAL\n",
    "inf_diaria = pd.read_csv(\"./recarga_directa/Infilt_diaria.csv\", encoding='latin-1')\n",
    "inf_area = pd.read_csv(\"./recarga_directa/Infilt_area.csv\", encoding='latin-1')\n",
    "tasa_nat = Mod_utils.recarga_mensual(inf_diaria,inf_area)\n",
    "tasa_nat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e120f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASA RECARGA POZAS\n",
    "#1° el area total de pozas y acopios para la conversion de la tasa de l/s a m3/d\n",
    "icount = 0\n",
    "imop = 0\n",
    "isop = 0\n",
    "\n",
    "for irow in range(nrow):\n",
    "    for icol in range(ncol):\n",
    "        for ipol in range(areas_recarga_pozas.shape[0]):\n",
    "            icount += areas_recarga_pozas[ipol,irow,icol]\n",
    "        for imo in range(areas_recarga_acopio_m.shape[0]):\n",
    "            imop += areas_recarga_acopio_m[imo,irow,icol]\n",
    "        for iso in range(areas_recarga_acopio_s.shape[0]):\n",
    "            isop += areas_recarga_acopio_s[iso,irow,icol]\n",
    "icount\n",
    "\n",
    "#with open ('area_total_pozas.csv','w') as f:  #se guarda en csv\n",
    "#    f = csv.writer(f,lineterminator = '\\n')\n",
    "#    f.writerow(['Zona', 'Area_m2'])\n",
    "#    f.writerow(['1',str(icount)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b64e45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ahora dividir cada dato de la infiltracion por el area total y por los días del mes\n",
    "inf_mensual_pozas = pd.read_csv(\"./recarga_directa/Infiltracion_mensual.csv\", encoding='latin-1')\n",
    "\n",
    "inf_mensual_pozas.Meses = pd.to_datetime(inf_mensual_pozas.Meses, format='%d/%m/%Y')\n",
    "inf_mensual_pozas.set_index('Meses', inplace=True)\n",
    "\n",
    "df_mes_R = inf_mensual_pozas.reset_index() \n",
    "df_fecha = df_mes_R.iloc[0:,0]\n",
    "df_fecha = df_fecha.apply(lambda t: pd.Period(t, freq='S').days_in_month).tolist() #se pasa a lista\n",
    "inv_fecha = [1/i for i in df_fecha] #se calcula el inverso para futuros calculos\n",
    "\n",
    "#t = inf_mensual_pozas/icount #se divide por el area total de la zona\n",
    "inf_mensual_pozas.Inf *=(1/icount)\n",
    "inf_mensual_pozas.MOP *=(1/imop)\n",
    "inf_mensual_pozas.SOP *=(1/isop)\n",
    "\n",
    "tasa_p = inf_mensual_pozas.mul(inv_fecha, axis=0)\n",
    "\n",
    "#tasa_p.head()\n",
    "###verificar si funciona hace los calculos para las 3 columnas\n",
    "tasa_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557005bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Calculamos la tasa por cada celda considerando el ponderado por area de cada zona\n",
    "\n",
    "tasa_pond = np.zeros((nper,nrow,ncol),dtype=float)\n",
    "\n",
    "for isp in range (nper):\n",
    "    for irow in range(nrow):\n",
    "        for icol in range(ncol):\n",
    "            zona_rn = str(int(zonas_recarga[0,irow,icol]))\n",
    "            itasa = tasa_nat[zona_rn][isp] #tasa recarga natural en el nodo\n",
    "            iarea_pozas = areas_arr_s[0,irow,icol] # area total de pozas en el nodo\n",
    "            iarea_ac_m = areas_arr_mop[0,irow,icol] # area total de pozas en el nodo\n",
    "            iarea_ac_s = areas_arr_sop[0,irow,icol] # area total de pozas en el nodo\n",
    "            iarea_nat = Area_nodo - iarea_pozas - iarea_ac_m - iarea_ac_s\n",
    "            itasa_pozas = tasa_p[\"Inf\"][isp] #tasa pozas\n",
    "            itasa_aco_m = tasa_p[\"MOP\"][isp] #tasa acopios MOP\n",
    "            itasa_aco_s = tasa_p[\"SOP\"][isp] #tasa acopios MOP            \n",
    "            itasa_pond = (itasa * iarea_nat + itasa_pozas * iarea_pozas + itasa_aco_m * iarea_ac_m + itasa_aco_s * iarea_ac_s) / Area_nodo\n",
    "            tasa_pond[isp,irow,icol] = itasa_pond\n",
    "         \n",
    "        \n",
    "\n",
    "tasa_pond.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4595eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(15,15))\n",
    "figure1 = ax.imshow(tasa_pond[71]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc268246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escribir paquete RCH\n",
    "with open(os.path.join(model_ws,modelname+\".rch\"),\"w\") as f:\n",
    "    f.write(\"# MODFLOW-USGs Recharge Package\\n\")\n",
    "    f.write(\" 3 50\\n\")\n",
    "    for per in range(nper):\n",
    "        f.write(\" 1\\n\") #si tiene concentracion, hay que poner CONC al lado de este 1, y desúes de la tasa de recarga, agregar la concentración con el mismo formato\n",
    "        f.write(\"INTERNAL  1.000000e+000  (FREE)  -1  RECHARGE\\n\")\n",
    "        cont=1\n",
    "        for i in range(nrow):\n",
    "            for j in range(ncol):\n",
    "                a=\"{:e}\".format(tasa_pond[per][i][j])\n",
    "                f.write(str(a+\" \"))\n",
    "                if cont % 10==0:\n",
    "                    f.write(\"\\n\")\n",
    "                cont=cont+1\n",
    "            f.write(\"\\n\")\n",
    "            cont=1\n",
    "        #f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500cea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se actualiza el archivo .nam agregando el paquete de ets\n",
    "nam_file = modelname + \".nam\"\n",
    "rch_unit_number = 19\n",
    "\n",
    "rch_file = modelname+\".rch\"\n",
    "\n",
    "with open(os.path.join(model_ws,nam_file),\"a\") as f:\n",
    "    f.write(\"RCH \"+str(rch_unit_number)+\" \"+rch_file+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ece98",
   "metadata": {},
   "source": [
    "# Evaporación\n",
    "importar zonas de evaporación al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce181328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def areas_evt(sim,shp_rdir_obj,nrow,ncol,noflow):\n",
    "\n",
    "    shapeRecs = shp_rdir_obj.shapeRecords()\n",
    "    ix = GridIntersect(sim.modelgrid, method=\"vertex\", rtree=True)\n",
    "\n",
    "    #arreglo que contendrá que cantidad de cada zona de recarga (en área) hay en cada celda\n",
    "    areas_arr = np.zeros((len(shapeRecs),nrow,ncol), dtype=float) \n",
    "\n",
    "    for ishape in range(len(shapeRecs)):\n",
    "        shape = shapeRecs[ishape].shape \n",
    "        polyg_points = shape.points #pasar a puntos\n",
    "        p = Polygon(shell=polyg_points) #volver a polígonos\n",
    "        result = ix.intersect(p) #intersectar \n",
    "        cellids = result['cellids']\n",
    "        areas = result['areas']\n",
    "        for icell in range(len(cellids)):\n",
    "            if noflow[0,cellids[icell][0],cellids[icell][1]]==0:\n",
    "                areas_arr[0,cellids[icell][0],cellids[icell][1]]= 0\n",
    "            else:\n",
    "                areas_arr[ishape,cellids[icell][0],cellids[icell][1]]=areas[icell]\n",
    "\n",
    "    #arreglo que contendrá la zona de recarga en cada celda \n",
    "    areas_arr_s = np.zeros((1,nrow,ncol), dtype=int) \n",
    "    \n",
    "    #arreglo que contendrá la cantidad de area de las zonas de recarga para cada celda \n",
    "    areas=np.zeros((1,areas_arr.shape[0]), dtype=int) \n",
    "    \n",
    "    #Dado que los SHP ingresados tienen más zonas que las de evaporación (por el oren que tienen), posteriormente \n",
    "    #se asigna la correspondencia entre las zonas de los SHP (ipol), con las zonas de EVT\n",
    "    for irow in range(nrow):\n",
    "        for icol in range(ncol):\n",
    "            for ipol in range(areas_arr.shape[0]):\n",
    "                areas[0,ipol] = areas_arr[ipol,irow,icol]\n",
    "            areas_arr_s[0,irow,icol]=(np.argmax(areas,axis=1)+2)*noflow[0,irow,icol]\n",
    "            if areas_arr_s[0,irow,icol]==4:\n",
    "                areas_arr_s[0,irow,icol]=3*noflow[0,irow,icol]\n",
    "            elif areas_arr_s[0,irow,icol]==2:\n",
    "                areas_arr_s[0,irow,icol]=4*noflow[0,irow,icol]\n",
    "            elif areas_arr_s[0,irow,icol]>=5:\n",
    "                areas_arr_s[0,irow,icol]=2*noflow[0,irow,icol]\n",
    "                \n",
    "    return areas_arr_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a12be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_ETS(model_ws,modelname,top,nrow,ncol,ETS_series,areas_arr_s):\n",
    "    #Escribir paquete ETS. forma extendida\n",
    "    with open(os.path.join(model_ws,modelname+\".ets\"),\"w\") as f:\n",
    "        f.write(\"# MODFLOW-USGs Evapotranspiration Package\\n\")\n",
    "        f.write(\" 1 50 0  6  0\\n\")\n",
    "\n",
    "        cont=1\n",
    "        for i in range(nper):\n",
    "            f.write(\"  1 1 1 -1 1\\n\")\n",
    "            f.write(\"INTERNAL  1.000000e+000  (FREE)  -1  ET Surface\\n\")\n",
    "            cont=1\n",
    "            for i in range(nrow):\n",
    "                for j in range(ncol):\n",
    "                    a=\"{:e}\".format(top[i][j])\n",
    "                    f.write(str(a)+\" \")\n",
    "                    if (cont)%10==0:\n",
    "                        f.write(\"\\n\")\n",
    "                    cont=cont+1\n",
    "                f.write(\"\\n\")\n",
    "                cont=1\n",
    "            #f.write(\"\\n\")\n",
    "        \n",
    "            f.write(\"INTERNAL  1.000000e+000  (FREE)  -1  ET Rate\\n\")\n",
    "            cont=1\n",
    "            for i in range(nrow):\n",
    "                for j in range(ncol):\n",
    "                    if areas_arr_s[0][i][j]==2:\n",
    "                        a=\"{:e}\".format(ETS_series[0][1])\n",
    "                    elif areas_arr_s[0][i][j]==3:\n",
    "                        a=\"{:e}\".format(ETS_series[0][2]) \n",
    "                    elif areas_arr_s[0][i][j]==4:\n",
    "                        a=\"{:e}\".format(ETS_series[0][3])\n",
    "                    else:\n",
    "                        a=\"{:e}\".format(0)\n",
    "                    f.write(str(a)+\" \")\n",
    "                    if (cont)%10==0:\n",
    "                        f.write(\"\\n\")\n",
    "                    cont=cont+1\n",
    "                f.write(\"\\n\")\n",
    "                cont=1\n",
    "            #f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"INTERNAL  1.000000e+000  (FREE)  -1  Extinction Depth\\n\")\n",
    "            cont=1\n",
    "            for i in range(nrow):\n",
    "                for j in range(ncol):\n",
    "                    if areas_arr_s[0][i][j]==2:\n",
    "                        a=\"{:e}\".format(ETS_series[1][1])\n",
    "                    elif areas_arr_s[0][i][j]==3:\n",
    "                        a=\"{:e}\".format(ETS_series[1][2]) \n",
    "                    elif areas_arr_s[0][i][j]==4:\n",
    "                        a=\"{:e}\".format(ETS_series[1][3])\n",
    "                    else:\n",
    "                        a=\"{:e}\".format(0)\n",
    "                    f.write(str(a)+\" \")\n",
    "                    if (cont)%10==0:\n",
    "                        f.write(\"\\n\")\n",
    "                    cont=cont+1\n",
    "                f.write(\"\\n\")\n",
    "                cont=1\n",
    "            #f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"INTERNAL  1.000000e+000  (FREE)  -1  PXDP0\\n\")\n",
    "            cont=1\n",
    "            for i in range(nrow):\n",
    "                for j in range(ncol):\n",
    "                    if areas_arr_s[0][i][j]==2:\n",
    "                        a=\"{:e}\".format(ETS_series[2][1])\n",
    "                    elif areas_arr_s[0][i][j]==3:\n",
    "                        a=\"{:e}\".format(ETS_series[2][2]) \n",
    "                    elif areas_arr_s[0][i][j]==4:\n",
    "                        a=\"{:e}\".format(ETS_series[2][3])\n",
    "                    else:\n",
    "                        a=\"{:e}\".format(0)\n",
    "                    f.write(str(a)+\" \")\n",
    "                    if (cont)%10==0:\n",
    "                        f.write(\"\\n\")\n",
    "                    cont=cont+1\n",
    "                f.write(\"\\n\")\n",
    "                cont=1\n",
    "            #f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"INTERNAL  1.000000e+000  (FREE)  -1  PETM0\\n\")\n",
    "            cont=1\n",
    "            for i in range(nrow):\n",
    "                for j in range(ncol):\n",
    "                    if areas_arr_s[0][i][j]==2:\n",
    "                        a=\"{:e}\".format(ETS_series[3][1])\n",
    "                    elif areas_arr_s[0][i][j]==3:\n",
    "                        a=\"{:e}\".format(ETS_series[3][2]) \n",
    "                    elif areas_arr_s[0][i][j]==4:\n",
    "                        a=\"{:e}\".format(ETS_series[3][3])\n",
    "                    else:\n",
    "                        a=\"{:e}\".format(0)\n",
    "                    f.write(str(a)+\" \")\n",
    "                    if (cont)%10==0:\n",
    "                        f.write(\"\\n\")\n",
    "                    cont=cont+1\n",
    "                f.write(\"\\n\")\n",
    "                cont=1\n",
    "            #f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"INTERNAL  1.000000e+000  (FREE)  -1  PXDP1\\n\")\n",
    "            cont=1\n",
    "            for i in range(nrow):\n",
    "                for j in range(ncol):\n",
    "                    if areas_arr_s[0][i][j]==2:\n",
    "                        a=\"{:e}\".format(ETS_series[4][1])\n",
    "                    elif areas_arr_s[0][i][j]==3:\n",
    "                        a=\"{:e}\".format(ETS_series[4][2]) \n",
    "                    elif areas_arr_s[0][i][j]==4:\n",
    "                        a=\"{:e}\".format(ETS_series[4][3])\n",
    "                    else:\n",
    "                        a=\"{:e}\".format(0)\n",
    "                    f.write(str(a)+\" \")\n",
    "                    if (cont)%10==0:\n",
    "                        f.write(\"\\n\")\n",
    "                    cont=cont+1\n",
    "                f.write(\"\\n\")\n",
    "                cont=1\n",
    "            #f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"INTERNAL  1.000000e+000  (FREE)  -1  PETM1\\n\")\n",
    "            cont=1\n",
    "            for i in range(nrow):\n",
    "                for j in range(ncol):\n",
    "                    if areas_arr_s[0][i][j]==2:\n",
    "                        a=\"{:e}\".format(ETS_series[5][1])\n",
    "                    elif areas_arr_s[0][i][j]==3:\n",
    "                        a=\"{:e}\".format(ETS_series[5][2]) \n",
    "                    elif areas_arr_s[0][i][j]==4:\n",
    "                        a=\"{:e}\".format(ETS_series[5][3])\n",
    "                    else:\n",
    "                        a=\"{:e}\".format(0)\n",
    "                    f.write(str(a)+\" \")\n",
    "                    if (cont)%10==0:\n",
    "                        f.write(\"\\n\")\n",
    "                    cont=cont+1\n",
    "                f.write(\"\\n\")\n",
    "                cont=1\n",
    "            #f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"INTERNAL  1.000000e+000  (FREE)  -1  PXDP2\\n\")\n",
    "            cont=1\n",
    "            for i in range(nrow):\n",
    "                for j in range(ncol):\n",
    "                    if areas_arr_s[0][i][j]==2:\n",
    "                        a=\"{:e}\".format(ETS_series[6][1])\n",
    "                    elif areas_arr_s[0][i][j]==3:\n",
    "                        a=\"{:e}\".format(ETS_series[6][2]) \n",
    "                    elif areas_arr_s[0][i][j]==4:\n",
    "                        a=\"{:e}\".format(ETS_series[6][3])\n",
    "                    else:\n",
    "                        a=\"{:e}\".format(0)\n",
    "                    f.write(str(a)+\" \")\n",
    "                    if (cont)%10==0:\n",
    "                        f.write(\"\\n\")\n",
    "                    cont=cont+1\n",
    "                f.write(\"\\n\")\n",
    "                cont=1\n",
    "            #f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"INTERNAL  1.000000e+000  (FREE)  -1  PETM2\\n\")\n",
    "            cont=1\n",
    "            for i in range(nrow):\n",
    "                for j in range(ncol):\n",
    "                    if areas_arr_s[0][i][j]==2:\n",
    "                        a=\"{:e}\".format(ETS_series[7][1])\n",
    "                    elif areas_arr_s[0][i][j]==3:\n",
    "                        a=\"{:e}\".format(ETS_series[7][2]) \n",
    "                    elif areas_arr_s[0][i][j]==4:\n",
    "                        a=\"{:e}\".format(ETS_series[7][3])\n",
    "                    else:\n",
    "                        a=\"{:e}\".format(0)\n",
    "                    f.write(str(a)+\" \")\n",
    "                    if (cont)%10==0:\n",
    "                        f.write(\"\\n\")\n",
    "                    cont=cont+1\n",
    "                f.write(\"\\n\")\n",
    "                cont=1\n",
    "            #f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"INTERNAL  1.000000e+000  (FREE)  -1  PXDP3\\n\")\n",
    "            cont=1\n",
    "            for i in range(nrow):\n",
    "                for j in range(ncol):\n",
    "                    if areas_arr_s[0][i][j]==2:\n",
    "                        a=\"{:e}\".format(ETS_series[8][1])\n",
    "                    elif areas_arr_s[0][i][j]==3:\n",
    "                        a=\"{:e}\".format(ETS_series[8][2]) \n",
    "                    elif areas_arr_s[0][i][j]==4:\n",
    "                        a=\"{:e}\".format(ETS_series[8][3])\n",
    "                    else:\n",
    "                        a=\"{:e}\".format(0)\n",
    "                    f.write(str(a)+\" \")\n",
    "                    if (cont)%10==0:\n",
    "                        f.write(\"\\n\")\n",
    "                    cont=cont+1\n",
    "                f.write(\"\\n\")\n",
    "                cont=1\n",
    "            #f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"INTERNAL  1.000000e+000  (FREE)  -1  PETM3\\n\")\n",
    "            cont=1\n",
    "            for i in range(nrow):\n",
    "                for j in range(ncol):\n",
    "                    if areas_arr_s[0][i][j]==2:\n",
    "                        a=\"{:e}\".format(ETS_series[9][1])\n",
    "                    elif areas_arr_s[0][i][j]==3:\n",
    "                        a=\"{:e}\".format(ETS_series[9][2]) \n",
    "                    elif areas_arr_s[0][i][j]==4:\n",
    "                        a=\"{:e}\".format(ETS_series[9][3])\n",
    "                    else:\n",
    "                        a=\"{:e}\".format(0)\n",
    "                    f.write(str(a)+\" \")\n",
    "                    if (cont)%10==0:\n",
    "                        f.write(\"\\n\")\n",
    "                    cont=cont+1\n",
    "                f.write(\"\\n\")\n",
    "                cont=1\n",
    "            #f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"INTERNAL  1.000000e+000  (FREE)  -1  PXDP4\\n\")\n",
    "            cont=1\n",
    "            for i in range(nrow):\n",
    "                for j in range(ncol):\n",
    "                    if areas_arr_s[0][i][j]==2:\n",
    "                        a=\"{:e}\".format(ETS_series[10][1])\n",
    "                    elif areas_arr_s[0][i][j]==3:\n",
    "                        a=\"{:e}\".format(ETS_series[10][2]) \n",
    "                    elif areas_arr_s[0][i][j]==4:\n",
    "                        a=\"{:e}\".format(ETS_series[10][3])\n",
    "                    else:\n",
    "                        a=\"{:e}\".format(0)\n",
    "                    f.write(str(a)+\" \")\n",
    "                    if (cont)%10==0:\n",
    "                        f.write(\"\\n\")\n",
    "                    cont=cont+1\n",
    "                f.write(\"\\n\")\n",
    "                cont=1\n",
    "            #f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"INTERNAL  1.000000e+000  (FREE)  -1  PETM4\\n\")\n",
    "            cont=1\n",
    "            for i in range(nrow):\n",
    "                for j in range(ncol):\n",
    "                    if areas_arr_s[0][i][j]==2:\n",
    "                        a=\"{:e}\".format(ETS_series[11][1])\n",
    "                    elif areas_arr_s[0][i][j]==3:\n",
    "                        a=\"{:e}\".format(ETS_series[11][2]) \n",
    "                    elif areas_arr_s[0][i][j]==4:\n",
    "                        a=\"{:e}\".format(ETS_series[11][3])\n",
    "                    else:\n",
    "                        a=\"{:e}\".format(0)\n",
    "                    f.write(str(a)+\" \")\n",
    "                    if (cont)%10==0:\n",
    "                        f.write(\"\\n\")\n",
    "                    cont=cont+1\n",
    "                f.write(\"\\n\")\n",
    "                cont=1\n",
    "            #f.write(\"\\n\")\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0824fb9a",
   "metadata": {},
   "source": [
    "Main evaporación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_rdir_obj = shp.Reader(r\".\\EVT\\ETS_draw_nucleo_.shp\")\n",
    "\n",
    "# Funcion areas_evt\n",
    "areas_arr_s = areas_evt(sim,shp_rdir_obj,nrow,ncol,noflow)\n",
    "\n",
    "#Leer top del L1 y parametros de zonas de evaporación\n",
    "top=dis.gettop()\n",
    "ETS_series=pd.read_excel(r\".\\EVT\\EVT_parameters.xlsx\", 'to_python')\n",
    "\n",
    "#leer archivo de recargas\n",
    "ETS_series=ETS_series.to_numpy()\n",
    "\n",
    "# Se crea el paquete ets\n",
    "ets_file = modelname+\".ets\"\n",
    "\n",
    "# Escribir paquete ETS. forma extendida\n",
    "write_ETS(model_ws,modelname,top,nrow,ncol,ETS_series,areas_arr_s)\n",
    "\n",
    "# Se actualiza el archivo .nam agregando el paquete de ets\n",
    "nam_file = modelname + \".nam\"\n",
    "ets_unit_number = 16\n",
    "\n",
    "with open(os.path.join(model_ws,nam_file),\"a\") as f:\n",
    "    f.write(\"ETS \"+str(ets_unit_number)+\" \"+ets_file+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b183197",
   "metadata": {},
   "source": [
    "# Correr Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b70a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.run_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec549f02",
   "metadata": {},
   "source": [
    "# Revisar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41d489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Se cargan resultados de hds para modelo re-construido\n",
    "heads_file = modelname+\".hds\"\n",
    "hdsobj = bf.HeadFile(os.path.join(model_ws,heads_file))\n",
    "hds = hdsobj.get_alldata()\n",
    "hds.shape\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "\n",
    "vmin = 2290.\n",
    "vmax = 2304.\n",
    "cnt=range(2290,2304,2)\n",
    "layer=1\n",
    "mapview = flopy.plot.PlotMapView(model=sim,layer=layer)\n",
    "\n",
    "mapview.plot_array(hds[71][layer], cmap='jet',vmin=vmin, vmax=vmax)\n",
    "mapview.plot_grid(colors='k', alpha=0.1)\n",
    "contours = mapview.contour_array(hds[len(hds)-1][layer], levels=cnt, linewidths=3., colors='black')\n",
    "mapview = flopy.plot.PlotMapView(model=sim,layer=layer)\n",
    "\n",
    "#mapview.contour_array(hds[len(hds)-1][layer], levels=[2290, 2292, 2294, 2296, 2298, 2300, 2302, 2304], linewidths=3.,\n",
    "#                 colors=['black','red','orange','Blue','Grey'],linestyles='dashed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0160a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_timex = time.time() - t0x\n",
    "print(\"Construcción y Tiempo Simulación Modelo: {:.3f} sec\".format(time.time() - t0x))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "816px",
    "left": "34px",
    "top": "111.638px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "571.369px",
    "left": "1762.99px",
    "right": "20px",
    "top": "146px",
    "width": "349.969px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
